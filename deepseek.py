import os  
import pdfplumber  
from typing import List, Optional, Dict, Any  

from langchain_community.embeddings import HuggingFaceBgeEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_core.documents import Document  
from langchain_openai import ChatOpenAI  
from langchain_core.prompts import ChatPromptTemplate  
from langchain_text_splitters import RecursiveCharacterTextSplitter  

os.environ['TOKENIZERS_PARALLELISM'] = 'false'  
# ğŸ” é…ç½® DeepSeek API  
os.environ["OPENAI_API_KEY"] = os.getenv("DEEPSEEK_API_KEY")  
os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"  

class PDFRAGAssistant:  
    def __init__(  
        self,   
        pdf_path: Optional[str] = None,   
        embedding_model: str = "BAAI/bge-large-zh-v1.5",  
        chunk_size: int = 300,  
        chunk_overlap: int = 50  
    ):  
        # 1. åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨  
        self.text_splitter = RecursiveCharacterTextSplitter(  
            chunk_size=chunk_size,  
            chunk_overlap=chunk_overlap,  
            length_function=len,  
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]  
        )  
        
        # 2. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹  
        self.embeddings = HuggingFaceBgeEmbeddings(  
            model_name=embedding_model,  
            model_kwargs={'device': 'cpu'}  
        )  
        
        # 3. åˆå§‹åŒ–æ–‡æ¡£å’Œå‘é‡å­˜å‚¨  
        self.docs = []  
        self.vector_store = None  
        
        # 4. åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹  
        self.llm = ChatOpenAI(  
            model="deepseek-chat",  
            temperature=0.7,  
            max_tokens=500,
            streaming=True  
        )  
        
        # 5. RAG æç¤ºæ¨¡æ¿  
        self.rag_prompt = ChatPromptTemplate.from_template(  
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œç”¨ä¸“ä¸šä¸”æ¸…æ™°çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚\n"  
            "æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼š{context}\n"  
            "å…·ä½“é—®é¢˜ï¼š{question}\n"  
            "è¦æ±‚ï¼š\n"  
            "1. ç›´æ¥å¼•ç”¨æ–‡æ¡£åŸæ–‡\n"  
            "2. å¦‚ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œè¯´æ˜åŸå› \n"  
            "3. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§"  
        )  
        
        # å¦‚æœæä¾›äº† PDF è·¯å¾„ï¼Œåˆ™åŠ è½½æ–‡æ¡£  
        if pdf_path:  
            self.load_pdf(pdf_path)  
    
    def load_pdf(self, pdf_path: str) -> List[Document]:  
        """ä» PDF æ–‡ä»¶åŠ è½½å’Œåˆ†å‰²æ–‡æ¡£"""  
        if not os.path.exists(pdf_path):  
            raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")  
        
        with pdfplumber.open(pdf_path) as pdf:  
            # æå–æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬  
            full_text = ""  
            for page in pdf.pages:  
                page_text = page.extract_text() or ""  
                full_text += page_text + "\n\n"  
            
            # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨å¤„ç†æ–‡æ¡£  
            docs = self.text_splitter.create_documents(  
                [full_text],   
                metadatas=[{"source": pdf_path}]  
            )  
        
        # æ›´æ–°æ–‡æ¡£å’Œå‘é‡å­˜å‚¨  
        self.docs = docs  
        self.vector_store = FAISS.from_documents(docs, self.embeddings)  
        
        return docs  
    
    def retrieve_relevant_docs(  
        self,   
        query: str,   
        k: int = 3,  
        score_threshold: float = 0.5  # é™ä½é˜ˆå€¼  
    ) -> Dict[str, Any]:  
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""  
        if not self.vector_store:  
            raise ValueError("è¯·å…ˆåŠ è½½ PDF æ–‡æ¡£")  
        
        # è¿›è¡Œç›¸ä¼¼æ€§æœç´¢  
        results = self.vector_store.similarity_search_with_score(query, k=k)  
        
        # æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹  
        context = "\n\n".join([  
            f"[ç›¸ä¼¼åº¦ {score:.2f}] {doc.page_content}"  
            for doc, score in results  
        ])  
        
        return {  
            "context": context,  
            "details": results  
        }  
    
    def generate_response(self, query: str) -> str:  
        """ç”Ÿæˆé—®é¢˜å›å¤"""  
        try:  
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£  
            retrieval_result = self.retrieve_relevant_docs(query)  
            context = retrieval_result['context']  
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£  
            if not context:  
                return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°é—®é¢˜ã€‚"  
            
            # æ„å»ºå®Œæ•´çš„æç¤º  
            prompt = self.rag_prompt.format(  
                context=context,   
                question=query  
            )  
            
              # æµå¼è°ƒç”¨è¯­è¨€æ¨¡å‹  
            full_response = ""  
            for chunk in self.llm.stream(prompt):  
                chunk_content = chunk.content  
                full_response += chunk_content  
                print(chunk_content, end='', flush=True)  
            
            print()  # æ¢è¡Œ  
            return full_response 
        
        except Exception as e:  
            return f"ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"  

def main():  
    # PDF æ–‡ä»¶è·¯å¾„  
    pdf_path = "./test.pdf"  
    
    # åˆ›å»º PDF RAG åŠ©æ‰‹  
    rag_assistant = PDFRAGAssistant(pdf_path)  
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨  
    questions = [  
        "è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",  
        "æ–‡æ¡£ä¸­æåˆ°äº†äººå·¥æ™ºèƒ½çš„å“ªäº›å…³é”®è§‚ç‚¹ï¼Ÿ",  
        "æ–‡æ¡£å¦‚ä½•æè¿°äººæœºåä½œï¼Ÿ"  
    ]  
    
    # é€ä¸ªå›ç­”é—®é¢˜  
    for q in questions:  
        print(f"é—®é¢˜: {q}")  
        print("å›ç­”:", rag_assistant.generate_response(q))  
        print("-" * 40)  

if __name__ == "__main__":  
    main()  