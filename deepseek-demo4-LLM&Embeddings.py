import os
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ğŸ” é…ç½® DeepSeek API
os.environ["OPENAI_API_KEY"] = os.getenv("DEEPSEEK_API_KEY")
os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"

class RAGAssistant:
    def __init__(self):
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = HuggingFaceBgeEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={'device': 'cpu'}
        )
        
        # 2. å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£
        self.docs = [
            Document(page_content="äººå·¥æ™ºèƒ½æ˜¯ä¸€ç§æ¨¡ä»¿äººç±»æ™ºèƒ½çš„æŠ€æœ¯ï¼Œå¯ä»¥å­¦ä¹ å’Œè§£å†³å¤æ‚é—®é¢˜"),
            Document(page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œé€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹æ¥æé«˜æ€§èƒ½"),
            Document(page_content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ä»»åŠ¡"),
            Document(page_content="å¤§è¯­è¨€æ¨¡å‹å¦‚GPTé€šè¿‡æµ·é‡æ–‡æœ¬å­¦ä¹ ï¼Œèƒ½å¤Ÿç”Ÿæˆäººç±»ç±»ä¼¼çš„æ–‡æœ¬"),
            Document(page_content="è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒï¼Œåœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨")
        ]
        
        # 3. åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = FAISS.from_documents(self.docs, self.embeddings)
        
        # 4. åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            temperature=0.7,
            max_tokens=300
        )
        
        # 5. è®¾è®¡ RAG æç¤ºæ¨¡æ¿
        self.rag_prompt = ChatPromptTemplate.from_template(
            "ä½ æ˜¯ä¸€ä¸ªAIæŠ€æœ¯ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\n"
            "ä¸Šä¸‹æ–‡ï¼š{context}\n"
            "é—®é¢˜ï¼š{question}\n"
            "å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å›ç­”ã€‚"
        )
    
    def retrieve_relevant_docs(self, query):
        """æ–‡æ¡£æ£€ç´¢ - æ‰¾æœ€ç›¸å…³çš„æ–‡æ¡£ ğŸ”"""
        # ç¡®ä¿ä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²
        if not isinstance(query, str):
            query = str(query)
        
        # è¿›è¡Œç›¸ä¼¼æ€§æœç´¢
        results = self.vector_store.similarity_search(query, k=2)
        
        # æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹
        context = "\n".join([doc.page_content for doc in results])
        return context
    
    def generate_response(self, query):
        """ç”Ÿæˆå›å¤çš„æ ¸å¿ƒæ–¹æ³•"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = self.retrieve_relevant_docs(query)
        
        # æ„å»ºå®Œæ•´çš„æç¤º
        prompt = self.rag_prompt.format(
            context=context, 
            question=query
        )
        
        # è°ƒç”¨è¯­è¨€æ¨¡å‹
        response = self.llm.invoke(prompt)
        
        # è¿”å›æ–‡æœ¬å†…å®¹
        return response.content

def main():
    # åˆ›å»º RAG åŠ©æ‰‹
    rag_assistant = RAGAssistant()
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ"
    ]
    
    # é€ä¸ªå›ç­”é—®é¢˜
    for q in questions:
        print(f"é—®é¢˜: {q}")
        print("å›ç­”:", rag_assistant.generate_response(q))
        print("-" * 40)

if __name__ == "__main__":
    main()